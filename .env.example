# TAI-EvalGenTCS CLI Configuration
# Copy this file to .env and fill in your values

# ============================================
# OpenRouter API Configuration
# ============================================
# Get your API key from: https://openrouter.ai/keys
OPENROUTER_API_KEY=your_openrouter_api_key_here

# OpenRouter API base URL (usually no need to change)
OPENROUTER_API_BASE=https://openrouter.ai/api/v1

# ============================================
# LLM Model Configuration
# ============================================
# Available models (via OpenRouter):
# - openai/gpt-4.1-mini (recommended for balance of cost/quality)
# - openai/gpt-4.1-nano (faster, lower cost)
# - google/gemini-2.5-flash (alternative option)
# - openai/gpt-4-turbo (higher quality, higher cost)
LLM_MODEL=openai/gpt-4.1-mini

# Temperature for LLM responses (0.0 = deterministic, 1.0 = creative)
# Use 0.0 for maximum determinism and consistent results across runs
# Higher values (0.1-0.3) allow some variation but may reduce consistency
LLM_TEMPERATURE=0.0

# Maximum tokens for LLM response (adjust based on test file size)
# For small test files (< 100 lines): 8000
# For medium test files (100-200 lines): 16000
# For large test files (> 200 lines): 32000 or higher
# Note: Some models have lower output limits (e.g., Claude Haiku: ~4096 tokens)
LLM_MAX_TOKENS=16000

# Timeout for LLM requests in seconds (default: 300 = 5 minutes)
# Increase if using slower models
LLM_TIMEOUT=300

# Seed for deterministic sampling (optional)
# Set to any integer (e.g., 42, 12345) to get consistent results across runs
# Leave unset or comment out for non-deterministic behavior
# Note: Not all models support seed parameter (OpenAI models do)
LLM_SEED=42
# ============================================
# Rate Limiting Configuration
# ============================================
# Based on OpenRouter limits: https://openrouter.ai/docs/api-reference/limits
# For credit balance > $15:
RATE_LIMIT_REQUESTS_PER_MINUTE=60
RATE_LIMIT_TOKENS_PER_MINUTE=100000

# ============================================
# Retry Configuration
# ============================================
# Number of retry attempts on API failure
RETRY_ATTEMPTS=3

# Initial delay between retries (seconds)
RETRY_DELAY=2.0

# Backoff factor for exponential backoff
# Delay will be: RETRY_DELAY * (BACKOFF_FACTOR ^ attempt)
BACKOFF_FACTOR=3.0

# ============================================
# Application Configuration
# ============================================
APP_NAME=TAI-EvalGenTCS
APP_VERSION=1.1.0

# Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO
