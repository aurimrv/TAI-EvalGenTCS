# TAI-EvalGenTCS CLI

**Test AI Evaluator and Generator of Test Case Suites - Command Line Interface**

Ferramenta de linha de comando para avaliaÃ§Ã£o e melhoria de suÃ­tes de teste baseada em 25 boas prÃ¡ticas de engenharia de software, desenvolvida como parte da pesquisa de doutorado na **Universidade Federal de SÃ£o Carlos (UFSCar)**.

## ğŸ“‹ Sobre o Projeto

Esta ferramenta implementa os resultados da tese de doutorado *"Towards a strategy and tool support for test generation based on good software testing practices: classification and prioritization"* de **Camilo HernÃ¡n Villota Ibarra**, oferecendo uma interface de linha de comando para:

- **Avaliar** a conformidade de casos de teste com 25 boas prÃ¡ticas fundamentais
- **Melhorar** automaticamente suÃ­tes de teste com base nas boas prÃ¡ticas
- **Gerar relatÃ³rios** detalhados em formato JSON

### FundamentaÃ§Ã£o TeÃ³rica

A ferramenta estÃ¡ fundamentada em uma **RevisÃ£o SistemÃ¡tica da Literatura (SLR)** que:
- Identificou **131 prÃ¡ticas** de testes de software em 103 estudos primÃ¡rios
- Refinou e sintetizou essas prÃ¡ticas em **40 boas prÃ¡ticas essenciais**
- Validou empiricamente atravÃ©s de pesquisa com testers profissionais
- Implementa **25 boas prÃ¡ticas fundamentais** divididas em:
  - **Common Sense (CS)**: 14 prÃ¡ticas de senso comum validadas pela indÃºstria
  - **Literature Supported (LS)**: 11 prÃ¡ticas respaldadas por pesquisas acadÃªmicas

## ğŸ—ï¸ Arquitetura

O sistema utiliza uma **arquitetura multi-agente** com os seguintes componentes:

```
tai-evalgentcs-cli/
â”œâ”€â”€ main.py                          # Ponto de entrada CLI
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/                      # Agentes especializados
â”‚   â”‚   â”œâ”€â”€ test_analyzer_agent.py   # AnÃ¡lise de cÃ³digo de teste
â”‚   â”‚   â””â”€â”€ test_improver_agent.py   # GeraÃ§Ã£o de cÃ³digo melhorado
â”‚   â”œâ”€â”€ config/                      # ConfiguraÃ§Ãµes
â”‚   â”‚   â””â”€â”€ settings.py              # Gerenciamento de configuraÃ§Ãµes
â”‚   â”œâ”€â”€ models/                      # Modelos de dados
â”‚   â”‚   â””â”€â”€ practice_manager.py      # Gerenciamento de boas prÃ¡ticas
â”‚   â”œâ”€â”€ services/                    # ServiÃ§os
â”‚   â”‚   â”œâ”€â”€ llm_client.py            # Cliente LLM com rate limiting
â”‚   â”‚   â””â”€â”€ orchestrator.py          # OrquestraÃ§Ã£o do workflow
â”‚   â””â”€â”€ utils/                       # UtilitÃ¡rios
â”‚       â””â”€â”€ logger.py                # ConfiguraÃ§Ã£o de logging
â”œâ”€â”€ data/
â”‚   â””â”€â”€ best_practices.json          # DefiniÃ§Ãµes das 25 boas prÃ¡ticas
â”œâ”€â”€ requirements.txt                 # DependÃªncias Python
â””â”€â”€ .env.example                     # Template de configuraÃ§Ã£o
```

### Componentes Principais

- **TestAnalyzerAgent**: Analisa cÃ³digo de teste e avalia conformidade com boas prÃ¡ticas
- **TestImproverAgent**: Gera versÃµes melhoradas do cÃ³digo de teste
- **PracticeManager**: Gerencia as definiÃ§Ãµes das 25 boas prÃ¡ticas
- **LLMClient**: Cliente para comunicaÃ§Ã£o com OpenRouter API (rate limiting e retry)
- **TestEvaluationOrchestrator**: Coordena o fluxo de trabalho entre agentes

## ğŸš€ InstalaÃ§Ã£o

### Requisitos

- Python 3.11 ou superior
- Conta no [OpenRouter](https://openrouter.ai/)
- CrÃ©dito mÃ­nimo de $15 no OpenRouter (para rate limits adequados)

### Passos de InstalaÃ§Ã£o

1. **Clone o repositÃ³rio:**
```bash
git clone <repository-url>
cd tai-evalgentcs-cli
```

2. **Crie um ambiente virtual:**
```bash
python3 -m venv venv
source venv/bin/activate  # No Windows: venv\Scripts\activate
```

3. **Instale as dependÃªncias:**
```bash
pip install -r requirements.txt
```

4. **Configure as variÃ¡veis de ambiente:**
```bash
cp .env.example .env
# Edite .env e adicione sua chave de API do OpenRouter
```

## âš™ï¸ ConfiguraÃ§Ã£o

Edite o arquivo `.env` com suas configuraÃ§Ãµes:

```env
# OpenRouter API Key (obrigatÃ³rio)
OPENROUTER_API_KEY=your_openrouter_api_key_here

# Modelo LLM (recomendado: openai/gpt-4.1-mini)
LLM_MODEL=openai/gpt-4.1-mini

# Temperatura (0.0-1.0, recomendado: 0.1 para anÃ¡lise de cÃ³digo)
LLM_TEMPERATURE=0.1

# Rate Limiting (para crÃ©dito > $15)
RATE_LIMIT_REQUESTS_PER_MINUTE=60
RATE_LIMIT_TOKENS_PER_MINUTE=100000
```

## ğŸ“– Uso

### Modo 1: Verificar Conformidade com Boas PrÃ¡ticas

Gera um relatÃ³rio JSON detalhado sobre a conformidade do cÃ³digo de teste:

```bash
python main.py --check-best-practice \
  --original-test-set tests/UserServiceTest.java \
  --output-dir ./reports
```

**SaÃ­da:**
- `UserServiceTest_bp_report.json`: RelatÃ³rio completo em JSON

### Modo 2: Melhorar SuÃ­te de Testes

Gera uma versÃ£o melhorada do cÃ³digo de teste baseada nas boas prÃ¡ticas:

```bash
python main.py --improve-best-practice \
  --original-test-set tests/UserServiceTest.java \
  --output-dir ./improved
```

**SaÃ­da:**
- `UserServiceTest_improved.java`: CÃ³digo de teste melhorado
- `UserServiceTest_bp_report.json`: RelatÃ³rio completo em JSON
- `UserServiceTest_improvement_summary.md`: Resumo das melhorias

### OpÃ§Ãµes Adicionais

```bash
# Usar modelo LLM especÃ­fico
python main.py --check-best-practice \
  --original-test-set tests/UserServiceTest.java \
  --output-dir ./reports \
  --llm-model openai/gpt-4-turbo

# Ativar logging verbose
python main.py --check-best-practice \
  --original-test-set tests/UserServiceTest.java \
  --output-dir ./reports \
  --verbose

# Usar arquivo de configuraÃ§Ã£o customizado
python main.py --check-best-practice \
  --original-test-set tests/UserServiceTest.java \
  --output-dir ./reports \
  --config custom.env
```

## ğŸ“Š Formato do RelatÃ³rio

O relatÃ³rio JSON segue o schema definido em `report-schema.json` e inclui:

### Estrutura do RelatÃ³rio

```json
{
  "test_class_name": "UserServiceTest",
  "test_methods": [
    {
      "test_method_name": "testCreateUser",
      "practices_evaluation": [
        {
          "practice_code": "CS-01",
          "practice_title": "EspecificaÃ§Ã£o AtÃ´mica de Casos de Teste",
          "status": "âœ”ï¸",
          "justification": "O teste foca em um Ãºnico comportamento...",
          "original_code": null,
          "improved_code": null
        }
      ],
      "method_compliance_score": "92%",
      "suggested_code": "..."
    }
  ],
  "practices_report": [
    {
      "practice_code": "CS-01",
      "practice_title": "EspecificaÃ§Ã£o AtÃ´mica de Casos de Teste",
      "description": "...",
      "compliant_methods": 5,
      "non_compliant_methods": 1,
      "not_applicable_methods": 0,
      "total_methods": 6,
      "compliance_score": "83%"
    }
  ],
  "overall_compliance_score": "87%"
}
```

### Status de Conformidade

- **âœ”ï¸ (Atende)**: A prÃ¡tica Ã© seguida corretamente
- **âŒ (NÃ£o Atende)**: A prÃ¡tica nÃ£o Ã© seguida (com sugestÃ£o de melhoria)
- **âšª (NÃ£o AplicÃ¡vel)**: A prÃ¡tica nÃ£o se aplica ao contexto do teste

## ğŸ¯ As 25 Boas PrÃ¡ticas

### Common Sense Practices (CS-01 a CS-14)

1. **CS-01**: EspecificaÃ§Ã£o AtÃ´mica de Casos de Teste
2. **CS-02**: IndependÃªncia Completa de Casos de Teste
3. **CS-03**: Cobertura de Fluxos Normais e Excepcionais
4. **CS-04**: AnÃ¡lise de Valores Limite
5. **CS-05**: Modularidade Completa de Casos de Teste
6. **CS-06**: AnÃ¡lise Detalhada de Tamanho e Complexidade
7. **CS-07**: Design Complexo para DetecÃ§Ã£o de Falhas
8. **CS-08**: ManutenÃ§Ã£o Completa do CÃ³digo de Teste
9. **CS-09**: Rastreabilidade Completa de Casos de Teste
10. **CS-10**: Uso Rigoroso de Testes de Performance e SeguranÃ§a
11. **CS-11**: RevisÃ£o Regular de Casos de Teste
12. **CS-12**: CompreensÃ£o Clara de Casos de Teste
13. **CS-13**: Cobertura Estruturada do Processo de Teste
14. **CS-14**: Garantia Completa da Qualidade do CÃ³digo de Teste

### Literature Supported Practices (LS-01 a LS-11)

1. **LS-01**: UtilizaÃ§Ã£o Adequada de Cobertura de CÃ³digo
2. **LS-02**: UtilizaÃ§Ã£o NecessÃ¡ria de Testes Ausentes
3. **LS-03**: UtilizaÃ§Ã£o Eficiente de Cobertura de CÃ³digo
4. **LS-04**: Pegada Pequena de GeraÃ§Ã£o de CÃ³digo de Teste
5. **LS-05**: PriorizaÃ§Ã£o Completa do Design de Casos de Teste
6. **LS-06**: AdiÃ§Ã£o ResponsÃ¡vel de ManutenÃ§Ã£o de CÃ³digo de Teste
7. **LS-07**: UtilizaÃ§Ã£o Adequada de AsserÃ§Ãµes de Teste
8. **LS-08**: AdiÃ§Ã£o ResponsÃ¡vel de ComentÃ¡rios de DepuraÃ§Ã£o
9. **LS-09**: Design DeterminÃ­stico de Resultados de Teste
10. **LS-10**: Evitar Completamente Efeitos Colaterais de Teste
11. **LS-11**: UtilizaÃ§Ã£o Adequada de RÃ³tulos e Categorias

Detalhes completos em `data/best_practices.json`.

## ğŸ”§ Desenvolvimento

### Executar Testes

```bash
pytest tests/ -v --cov=src
```

### Estrutura de CÃ³digo Limpo

O projeto segue princÃ­pios de **Clean Architecture**:

- **SeparaÃ§Ã£o de responsabilidades**: Cada mÃ³dulo tem uma responsabilidade clara
- **InversÃ£o de dependÃªncias**: Componentes dependem de abstraÃ§Ãµes
- **Testabilidade**: CÃ³digo facilmente testÃ¡vel com mocks
- **ConfiguraÃ§Ã£o externa**: Todas as configuraÃ§Ãµes via `.env`

## ğŸ“š ReferÃªncias

- **Tese de Doutorado**: "Towards a strategy and tool support for test generation based on good software testing practices: classification and prioritization"
- **Autor**: Camilo HernÃ¡n Villota Ibarra
- **Orientador**: Prof. Dr. Auri Marcelo Rizzo Vincenzi
- **Co-orientador**: Prof. Dr. JosÃ© Carlos Maldonado
- **InstituiÃ§Ã£o**: Universidade Federal de SÃ£o Carlos (UFSCar)

## ğŸ“„ LicenÃ§a

Este projeto Ã© parte de uma pesquisa acadÃªmica na UFSCar.

## ğŸ‘¥ Autores

- **Camilo HernÃ¡n Villota Ibarra** - Autor Principal e Pesquisador
- **Auri Marcelo Rizzo Vincenzi** - Orientador
- **JosÃ© Carlos Maldonado** - Co-orientador

## ğŸ¤ ContribuiÃ§Ãµes

Para questÃµes, sugestÃµes ou contribuiÃ§Ãµes, entre em contato com os autores.
